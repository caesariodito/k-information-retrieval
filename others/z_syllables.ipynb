{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTV4jL4jmJmhLVAaMNurIo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvEZ2_NBEsMD","executionInfo":{"status":"ok","timestamp":1663162702655,"user_tz":-420,"elapsed":349,"user":{"displayName":"Caesario Dito","userId":"09026725535013084746"}},"outputId":"ee558d19-a080-4a64-bcda-b6b3182f825d"},"outputs":[{"output_type":"stream","name":"stdout","text":["no-syllable:  3\n","one-syllable:  87\n","two-syllable:  38\n","three-syllable:  14\n","four-syllable:  13\n","five or more-syllable:  9\n","\n","total syllable: 164\n","['study', 'whole', 'data', 'would', 'wise', 'extract', 'British', 'Corpus', 'would', 'select', 'sample', 'found', 'sample.', 'sample', 'allow', 'whole', 'been', 'used.', 'take', '(sample)', 'whole', 'corpus', 'tended,', 'where', 'sample', 'only', 'random', 'sample.', 'point', 'random', 'sample', 'each', 'chance', 'being', 'sample.', 'each', 'item', 'been']\n"]}],"source":["def syllable_count(str):\n","    count = 0\n","    \n","    syllables = set(\"AEIOUYaeiouy\")\n","    \n","    for letter in str:\n","        if letter in syllables:\n","            count = count + 1\n","      \n","    return count\n","\n","string = 'It is not always possible or practical to study a whole data set. Thus, if anyone is interested in analysing the preposition of, it would not be wise to extract all instances of this preposition from the British National Corpus (BNC hereafter). A positive solution would be to select a sample of the BNC and retrieve just the instances found in that sample. This sample might be enough to allow us to draw conclusions, probably as confidently as if the whole BNC had been used. That is, we take a representative portion (sample) of the whole corpus (population) for which gener-alization is in tended, and where the sample is representative only if it is a random sample. The point of a random sample is to ensure that each ítem or occurrence of the population has an equal chance of being selected for a sample. This eliminates biases as each item or occurrence is assumed to represent the population from which it has been retrieved.'\n","\n","splitted_str = str.split()\n","\n","none_s = 0\n","none = []\n","one_s = 0\n","one = []\n","two_s = 0\n","two = []\n","three_s = 0\n","three = []\n","four_s = 0\n","four = []\n","five_s = 0\n","five = []\n","\n","for x in splitted_str:\n","  if(syllable_count(x) == 0):\n","    none.append(x)\n","    none_s += 1\n","  elif(syllable_count(x) == 1):\n","    one_s += 1\n","    one.append(x)\n","  elif(syllable_count(x) == 2):\n","    two_s += 1\n","    two.append(x)\n","  elif(syllable_count(x) == 3):\n","    three_s += 1\n","    three.append(x)\n","  elif(syllable_count(x) == 4):\n","    four_s += 1\n","    four.append(x)\n","  elif(syllable_count(x) > 4):\n","    five_s += 1\n","    five.append(x)\n","\n","print('no-syllable: ', none_s)\n","print('one-syllable: ', one_s)\n","print('two-syllable: ', two_s)\n","print('three-syllable: ', three_s)\n","print('four-syllable: ', four_s)\n","print('five or more-syllable: ', five_s)\n","\n","print('\\ntotal syllable:', none_s + one_s + two_s + three_s + four_s + five_s)\n","\n","print(two)\n"]},{"cell_type":"code","source":["!pip install pyhyphen\n","!pip install syllables"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05eg8tHAO-9q","executionInfo":{"status":"ok","timestamp":1664074627993,"user_tz":-420,"elapsed":10908,"user":{"displayName":"Caesario Dito","userId":"09026725535013084746"}},"outputId":"66555def-d967-4e4d-d2f6-3b2bf658c794"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyhyphen\n","  Downloading PyHyphen-4.0.3.tar.gz (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.36.0 in /usr/local/lib/python3.7/dist-packages (from pyhyphen) (0.37.1)\n","Requirement already satisfied: setuptools>=52.0 in /usr/local/lib/python3.7/dist-packages (from pyhyphen) (57.4.0)\n","Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pyhyphen) (1.4.4)\n","Collecting requests>=2.25\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen) (2022.6.15)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen) (2.10)\n","Building wheels for collected packages: pyhyphen\n","  Building wheel for pyhyphen (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyhyphen: filename=PyHyphen-4.0.3-cp37-abi3-linux_x86_64.whl size=60309 sha256=3c194c81482738bfc97d993a8f1e05ba30124d2cc98431cd8ccbd5d848fb104c\n","  Stored in directory: /root/.cache/pip/wheels/4e/21/3e/e883a6e9969fdd074763213ddaeee0e781c359bbfda3fa435f\n","Successfully built pyhyphen\n","Installing collected packages: requests, pyhyphen\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","Successfully installed pyhyphen-4.0.3 requests-2.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting syllables\n","  Downloading syllables-1.0.3-py2.py3-none-any.whl (15 kB)\n","Installing collected packages: syllables\n","Successfully installed syllables-1.0.3\n"]}]},{"cell_type":"code","source":["import syllables\n","import pandas as pd\n","\n","text = 'It is not always possible or practical to study a whole data set. Thus, if anyone is interested in analysing the preposition of, it would not be wise to extract all instances of this preposition from the British National Corpus. A positive solution would be to select a sample of the BNC and retrieve just the instances found in that sample. This sample might be enough to allow us to draw conclusions, probably as confidently as if the whole BNC had been used. That is, we take a representative portion (sample) of the whole corpus (population) for which generalization is in tended, and where the sample is representative only if it is a random sample. The point of a random sample is to ensure that each ítem or occurrence of the population has an equal chance of being selected for a sample. This eliminates biases as each item or occurrence is assumed to represent the population from which it has been retrieved.'\n","\n","splitted_str = text.split()\n","\n","none_s = 0\n","none = []\n","one_s = 0\n","one = []\n","two_s = 0\n","two = []\n","three_s = 0\n","three = []\n","four_s = 0\n","four = []\n","five_s = 0\n","five = []\n","\n","for x in splitted_str:\n","  if(syllables.estimate(x) == 0):\n","    none.append(x)\n","    none_s += 1\n","  elif(syllables.estimate(x) == 1):\n","    one_s += 1\n","    one.append(x)\n","  elif(syllables.estimate(x) == 2):\n","    two_s += 1\n","    two.append(x)\n","  elif(syllables.estimate(x) == 3):\n","    three_s += 1\n","    three.append(x)\n","  elif(syllables.estimate(x) == 4):\n","    four_s += 1\n","    four.append(x)\n","  elif(syllables.estimate(x) > 4):\n","    five_s += 1\n","    five.append(x)\n","\n","print('no-syllable: ', none_s)\n","print('one-syllable: ', one_s)\n","print('two-syllable: ', two_s)\n","print('three-syllable: ', three_s)\n","print('four-syllable: ', four_s)\n","print('five or more-syllable: ', five_s)\n","\n","table = pd.DataFrame.from_dict({\n","    'one': one,\n","    'two': two,\n","    'three': three,\n","    'four': four,\n","    'five or more': five\n","}, orient='index')\n","\n","table = table.transpose()\n","\n","table\n","\n","table.to_csv('syllables.csv')\n","\n","# print(len(splitted_str))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzx8f5c2OjWA","executionInfo":{"status":"ok","timestamp":1664074673321,"user_tz":-420,"elapsed":712,"user":{"displayName":"Caesario Dito","userId":"09026725535013084746"}},"outputId":"8f63c96e-355c-4bef-ee35-a2325239b5b5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["no-syllable:  0\n","one-syllable:  100\n","two-syllable:  32\n","three-syllable:  15\n","four-syllable:  11\n","five or more-syllable:  4\n"]}]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acg0WXw0XsnU","executionInfo":{"status":"ok","timestamp":1663165196058,"user_tz":-420,"elapsed":3608,"user":{"displayName":"Caesario Dito","userId":"09026725535013084746"}},"outputId":"d2c2e3bc-a426-4cbe-9030-2e2a6c8e6d46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"]}]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","text = 'It is not always possible or practical to study a whole data set. Thus, if anyone is interested in analysing the preposition of, it would not be wise to extract all instances of this preposition from the British National Corpus. A positive solution would be to select a sample of the BNC and retrieve just the instances found in that sample. This sample might be enough to allow us to draw conclusions, probably as confidently as if the whole BNC had been used. That is, we take a representative portion (sample) of the whole corpus (population) for which generalization is in tended, and where the sample is representative only if it is a random sample. The point of a random sample is to ensure that each ítem or occurrence of the population has an equal chance of being selected for a sample. This eliminates biases as each item or occurrence is assumed to represent the population from which it has been retrieved.'\n","text = nltk.word_tokenize(text)\n","result = nltk.pos_tag(text)\n","# result = [i for i in result if i[0].lower() == 'table']\n","\n","# result\n","\n","# result[0][1]\n","\n","table = pd.DataFrame(result)\n","\n","\n","table.to_csv('nltk.csv')"],"metadata":{"id":"BOdYF7oSVDpu","executionInfo":{"status":"ok","timestamp":1664026200802,"user_tz":-420,"elapsed":1062,"user":{"displayName":"Caesario Dito","userId":"09026725535013084746"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"97be43dd-8d16-4624-fc6d-49f84080e6ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]}]}